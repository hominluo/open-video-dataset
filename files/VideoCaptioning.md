## Video Captioning
* **VideoStory**: A New Multimedia Embedding for Few-Example Recognition and Translation of Events </br>
[[Paper](https://isis-data.science.uva.nl/cgmsnoek/pub/habibian-videostory-mm2014.pdf)][[Homepage](https://ivi.fnwi.uva.nl/isis/mediamill/datasets/videostory.php)] </br>
*45,826 videos and their descriptions obtained by harvesting YouTube*

* **MSR-VTT**: A Large Video Description Dataset for Bridging Video and Language (CVPR 2016) </br>
[[Paper](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/cvpr16.msr-vtt.tmei_-1.pdf)][[Homepage](https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video-and-language/)] </br>
*10K web video clips, 200K clip-sentence pairs*

* **VaTeX**: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research (ICCV 2019) </br>
[[Paper](https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_VaTeX_A_Large-Scale_High-Quality_Multilingual_Dataset_for_Video-and-Language_Research_ICCV_2019_paper.pdf)][[Homepage](https://eric-xw.github.io/vatex-website/download.html)] </br>
*41,250 videos, 825,000 captions in both English and Chinese, over 206,000 English-Chinese parallel translation pairs*

* **ActivityNet Captions**: Dense-Captioning Events in Videos (ICCV 2017) </br>
[[Paper](https://arxiv.org/pdf/1705.00754.pdf)][[Homepage](https://cs.stanford.edu/people/ranjaykrishna/densevid/)] </br>
*20k videos, 100k sentences*

* **ActivityNet Entities**: Grounded Video Description </br>
[[Paper](https://arxiv.org/pdf/1812.06587.pdf)][[Homepage](https://github.com/facebookresearch/ActivityNet-Entities)] </br>
*14,281 annotated videos, 52k video segments with at least one noun phrase annotated per segment, augment the ActivityNet Captions dataset with 158k bounding box*

* **WebVid-2M**: Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval (2021) </br>
[[Paper](https://arxiv.org/pdf/2104.00650.pdf)][[Homepage](https://github.com/m-bain/frozen-in-time)] </br>
*over two million videos with weak captions scraped from the internet*

* **VTW**: Title Generation for User Generated Videos (ECCV 2016) </br>
[[Paper](https://arxiv.org/pdf/1608.07068.pdf)][[Homepage](http://aliensunmin.github.io/project/video-language/index.html#VideoTitle)] </br>
*18100 video clips with an average of 1.5 minutes duration per clip*

* **TGIF**: A New Dataset and Benchmark on Animated GIF Description (CVPR 2016) </br>
[[Paper](https://arxiv.org/abs/1604.02748)][[Homepage](http://raingo.github.io/TGIF-Release/)] </br>
*100K animated GIFs from Tumblr and 120K natural language descriptions*


* **Charades**: Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding (ECCV 2016)</br> 
[[Paper](http://ai2-website.s3.amazonaws.com/publications/hollywood-homes.pdf)][[Homepage](https://prior.allenai.org/projects/charades)]</br>
*9,848 annotated videos, 267 people, 27,847 video descriptions, 66,500 temporally localized intervals for 157 action classes and
41,104 labels for 46 object classes*
