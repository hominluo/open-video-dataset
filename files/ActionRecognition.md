## Action Recognition

* **HOLLYWOOD2**: Actions in Context (CVPR 2009) </br>
[[Paper](http://www.irisa.fr/vista/Papers/2009_cvpr_marszalek.pdf)][[Homepage](https://www.di.ens.fr/~laptev/actions/hollywood2/)]</br>
*12 classes of human actions, 10 classes of scenes, 3,669 clips, 69 movies*

* **HMDB**: A Large Video Database for Human Motion Recognition (ICCV 2011)</br> 
[[Paper](https://serre-lab.clps.brown.edu/wp-content/uploads/2012/08/Kuehne_etal_iccv11.pdf)][[Homepage](https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/)]</br>
*51 classes, 7,000 clips*

* **UCF101**: A Dataset of 101 Human Actions Classes From Videos in The Wild </br> 
[[Paper](https://www.crcv.ucf.edu/papers/UCF101_CRCV-TR-12-01.pdf)][[Homepage](https://www.crcv.ucf.edu/data/UCF101.php)]</br> 
*101 classes, 13k clips*

* **Sports-1M**: Large-scale Video Classification with Convolutional Neural Networks </br>
[[Paper](https://cs.stanford.edu/people/karpathy/deepvideo/deepvideo_cvpr2014.pdf)][[Homepage](https://cs.stanford.edu/people/karpathy/deepvideo/)]</br>
*1,000,000 videos, 487 classes*

* **ActivityNet**: A Large-Scale Video Benchmark for Human Activity Understanding (CVPR 2015)</br> 
[[Paper](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Heilbron_ActivityNet_A_Large-Scale_2015_CVPR_paper.pdf)][[Homepage](http://activity-net.org/index.html)]</br>
*203 classes, 137 untrimmed videos per class, 1.41 activity instances per video*

* **MPII-Cooking**: Recognizing Fine-Grained and Composite Activities Using Hand-Centric Features and Script Data (IJCV 2015) </br>
[[Paper](https://link.springer.com/content/pdf/10.1007/s11263-015-0851-8.pdf)][[Homepage](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/human-activity-recognition/mpii-cooking-2-dataset/)] </br>
*67 fine-grained activities, 59 composite activities, 14,105 clips, 273 videos*

* **Kinetics** </br>
[[Kinetics-400](https://arxiv.org/abs/1705.06950)/[Kinetics-600](https://arxiv.org/abs/1808.01340)/[Kinetics-700](https://arxiv.org/abs/1907.06987)/[Kinetics-700-2020](https://arxiv.org/pdf/2010.10864.pdf)] [[Homepage](https://deepmind.com/research/open-source/kinetics)]</br>
*400/600/700/700 classes, at least 400/600/600/700 clips per class*

* **Charades**: Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding (ECCV 2016)</br> 
[[Paper](http://ai2-website.s3.amazonaws.com/publications/hollywood-homes.pdf)][[Homepage](https://prior.allenai.org/projects/charades)]</br>
*9,848 annotated videos, 267 people, 27,847 video descriptions, 66,500 temporally localized intervals for 157 action classes and
41,104 labels for 46 object classes*

* **Toyota Smarthome**: Real-World Activities of Daily Living (ICCV 2019) </br>
[[Paper](https://openaccess.thecvf.com/content_ICCV_2019/papers/Das_Toyota_Smarthome_Real-World_Activities_of_Daily_Living_ICCV_2019_paper.pdf)][[Homepage](https://project.inria.fr/toyotasmarthome/)] </br>
*16,115 short RGB+D video samples, 31 activities, 3 modalities: RGB + Depth + 3D Skeleton, the subjects are senior people in the age range 60-80 years old*

* **Charades-Ego**: Actor and Observer: Joint Modeling of First and Third-Person Videos (CVPR 2018) </br> 
[[Paper](https://arxiv.org/pdf/1804.09627.pdf)][[Homepage](https://prior.allenai.org/projects/charades-ego)]</br>
*112 people, 4000 paired videos, 157 action classes*

* **20BN-jester**: The Jester Dataset: A Large-Scale Video Dataset of Human Gestures (ICCVW 2019) </br>
[[Paper](https://openaccess.thecvf.com/content_ICCVW_2019/papers/HANDS/Materzynska_The_Jester_Dataset_A_Large-Scale_Video_Dataset_of_Human_Gestures_ICCVW_2019_paper.pdf)][[Homepage](https://20bn.com/datasets/jester)] </br>
*148,092 videos, 27 classes, 1376 actors*

* **Moments in Time Dataset**: one million videos for event understanding (TPAMI 2019) </br> 
[[Paper](http://moments.csail.mit.edu/TPAMI.2019.2901464.pdf)][[Homepage](http://moments.csail.mit.edu/)]</br>
*over 1,000,000 labelled videos for 339 Moment classes, the average number of labeled videos per class is 1,757 with a median of 2,775*

* **Multi-Moments in Time**: Learning and Interpreting Models for Multi-Action Video Understanding </br> 
[[Paper](https://arxiv.org/pdf/1911.00232.pdf)][[Homepage](http://moments.csail.mit.edu/)]</br>
*1.02 million videos, 313 action classes, 553,535 videos are annotated with more than one label and 257,491 videos are annotated with three or more labels*

* **20BN-SOMETHING-SOMETHING**: The "something something" video database for learning and evaluating visual common sense </br> 
[[Paper](https://arxiv.org/abs/1706.04261)][[Homepage](https://20bn.com/datasets/something-something)]</br>
*100,000 videos across 174 classes*

* **EPIC-KITCHENS**: Scaling Egocentric Vision: The EPIC-KITCHENS Dataset (ECCV 2018, extended into TPAMI 2020)</br> 
[[Paper](https://arxiv.org/abs/2006.13256)][[Homepage](https://epic-kitchens.github.io/2021)]</br>
*100 hours, 37 participants, 20M frames, 90K action segments, 700 variable length videos, 97 verb classes, 300 noun classes, 4053 action classes*

* **HOMAGE**: Home Action Genome: Cooperative Compositional Action Understanding (CVPR 2021) </br> 
[[Paper]()][[Homepage](https://homeactiongenome.org/)]</br> 
*27 participants, 12 sensor types, 75 activities, 453 atomic actions, 1,752 synchronized sequences, 86 object classes, 29 relationship classes, 497,534 bounding boxes, 583,481 relationships*

* **MMAct**: A Large-Scale Dataset for Cross Modal Human Action Understanding (ICCV 2019) </br> 
[[Paper](https://openaccess.thecvf.com/content_ICCV_2019/papers/Kong_MMAct_A_Large-Scale_Dataset_for_Cross_Modal_Human_Action_Understanding_ICCV_2019_paper.pdf)][[Homepage](https://mmact19.github.io/2019/)]</br>
*36k video clips, 37 action classes, RGB+Keypoints+Acc+Gyo+Ori+Wi-Fi+Presure*

* **LEMMA**: A Multi-view Dataset for LEarning Multi-agent Multi-task Activities (ECCV 2020) </br> 
[[Paper](https://arxiv.org/pdf/2007.15781.pdf)][[Homepage](https://sites.google.com/view/lemma-activity)]</br>
*RGB-D, 641 action classes, 11,781 action segments, 4.6M frames*

* **NTU RGB+D**: A Large Scale Dataset for 3D Human Activity Analysis (CVPR 2016, TPAMI 2019) </br>
[[Paper](https://arxiv.org/abs/1604.02808)][[Homepage](http://rose1.ntu.edu.sg/Datasets/actionRecognition.asp)] </br>
*106 distinct subjects and contains more than 114 thousand video samples and 8 million frames, 120 action classes*

* **Action Genome**: Actions as Compositions of Spatio-temporal Scene Graphs (CVPR 2020) </br> 
[[Paper](https://arxiv.org/pdf/1912.06992.pdf)][[Homepage](https://www.actiongenome.org/)]</br>
*10K videos, 0.4M objects, 1.7M visual relationships*

* **TITAN**: Future Forecast using Action Priors (CVPR 2020) </br> 
[[Paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Malla_TITAN_Future_Forecast_Using_Action_Priors_CVPR_2020_paper.pdf)][[Homepage](https://usa.honda-ri.com/titan)]</br>
*700 labeled video-clips, 50 labels including vehicle states and actions, pedestrian age groups, and targeted pedestrian action attributes*

* **PKU-MMD**: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding (ACM Multimedia Workshop) </br> 
[[Paper](https://arxiv.org/abs/1703.07475)][[Homepage](https://github.com/ECHO960/PKU-MMD#pku-mmd-a-large-scale-benchmark-for-continuous-multi-modal-human-action-understanding)]</br>
*1,076 long video sequences, 51 action categories, performed by 66 subjects in three camera views, 20,000 action instances, 5.4 million frames, RGB+depth+Infrared Radiation+Skeleton*

* **HACS**: Human Action Clips and Segments Dataset for Recognition and Temporal Localization </br>
[[Paper](https://arxiv.org/pdf/1712.09374.pdf)][[Homepage](http://hacs.csail.mit.edu/)]</br>
*HACS Clips: 1.5M annotated clips sampled from 504K untrimmed videos, HACS Segments: 139K action segments densely annotated in 50K untrimmed videos spanning 200 action categories*

* **Oops!**: Predicting Unintentional Action in Video (CVPR 2020) </br>
[[Paper](https://arxiv.org/pdf/1911.11206.pdf)][[Homepage](https://oops.cs.columbia.edu/)] </br>
*20,338 videos, 7,368 annotated for training, 6,739 annotated for testing*

* **RareAct**: A video dataset of unusual interactions </br>
[[Paper](https://arxiv.org/pdf/2008.01018.pdf)][[Homepage](https://github.com/antoine77340/RareAct)]</br>
*122 different actions, 7,607 clips, 905 videos, 19 verbs, 38 nouns*

* **FineGym**: A Hierarchical Video Dataset for Fine-grained Action Understanding (CVPR 2020) </br>
[[Paper](https://arxiv.org/pdf/2004.06704.pdf)][[Homepage](https://sdolivia.github.io/FineGym/)] </br>
*10 event categories, including 6 male events and 4 female events, 530 element categories*

* **THUMOS**: The THUMOS challenge on action recognition for videos “in the wild” </br>
[[Paper](https://www.crcv.ucf.edu/papers/thumosCVIU.pdf)][[Homepage](http://crcv.ucf.edu/THUMOS14/)] </br>
*101 actions, train: 13,000 temporally trimmed videos, validation: 2100 temporally untrimmed videos with temporal annotations of actions, background: 3000 relevant videos, test: 5600 temporally untrimmed videos with withheld ground truth*

* **MultiTHUMOS**: Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos (IJCV 2017) </br>
[[Paper](https://arxiv.org/pdf/1507.05738.pdf)][[Homepage](http://ai.stanford.edu/~syyeung/everymoment.html)] </br>
*400 videos, 38,690 annotations of 65 action classes, 10.5 action classes per video*

* **TinyVIRAT**: Low-resolution Video Action Recognition </br>
[[Paper](https://www.crcv.ucf.edu/wp-content/uploads/2020/07/Publications_TinyVIRAT.pdf)][[Homepage](https://www.crcv.ucf.edu/research/projects/tinyvirat-low-resolution-video-action-recognition/)] </br>
*12,829 low-resolution videos, 26 classes, multi-label classification*

* **UAV-Human**: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles </br>
[[Paper](https://arxiv.org/abs/2104.00946)][[Homepage](https://github.com/SUTDCV/UAV-Human)] </br>
*67,428 multi-modal video sequences and 119 subjects for action recognition, 22,476 frames for pose estimation, 41,290 frames and 1,144 identities for person re-identification, and 22,263 frames for attribute recognition*

* **Hierarchical Action Search**: Searching for Actions on the Hyperbole (CVPR 2020) </br>
[[Paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Long_Searching_for_Actions_on_the_Hyperbole_CVPR_2020_paper.pdf)][[Homepage](https://github.com/Tenglon/hyperbolic_action)] </br>
*Hierarchical-ActivityNet, Hierarchical-Kinetics, and Hierarchical-Moments from ActivityNet, mini-Kinetics, and Moments-in-time; provide action
hierarchies and action splits for unseen action search*
